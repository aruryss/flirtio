name,model_name,learning_rate,batch_size,dropout,warmup_steps,epochs,max_length,weight_decay,freeze_bert,notes,best_train_accuracy,final_train_accuracy,best_val_accuracy,best_val_f1,best_val_precision,best_val_recall,overfitting_gap
conservative_stable,distilbert-base-uncased,2e-05,16,0.2,100,6,128,0.01,False,Safe baseline - should reach 80-85% easily,0.8068487879953828,0.8068487879953828,0.7953321364452424,0.7936357072402268,0.8051067544333541,0.7953321364452424,0.011516651550140478
optimized_learning,distilbert-base-uncased,3e-05,16,0.25,150,5,128,0.01,False,Balanced approach with proven settings,0.7360523278183917,0.7360523278183917,0.7737881508078994,0.7734185808876773,0.7754755617123055,0.7737881508078994,-0.03773582298950773
fast_convergence,distilbert-base-uncased,4e-05,24,0.2,200,5,128,0.01,False,Faster training with careful controls,0.6467872258560985,0.6467872258560985,0.755834829443447,0.7558190882260972,0.7558813207246879,0.755834829443447,-0.10904760358734855
high_performance,distilbert-base-uncased,2e-05,16,0.3,200,6,128,0.02,False,Maximum regularization for best generalization,0.6952674105425164,0.6952674105425164,0.7414721723518851,0.7414121597019965,0.7416582622695509,0.7414721723518851,-0.04620476180936872
two_stage_training,distilbert-base-uncased,3e-05,16,0.25,150,4,128,0.01,True,"Train classifier first, then fine-tune BERT",0.49403616775682957,0.49403616775682957,0.4991023339317774,0.36548213540969815,0.4817014783975921,0.49730700179533216,-0.005066166174947817
